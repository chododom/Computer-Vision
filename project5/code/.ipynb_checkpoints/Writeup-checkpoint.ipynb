{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writeup\n",
    "\n",
    "For this project, I have partnered up with Jan Dolejs in order to tackle the problem of feature matching. The goal was to compare two images of the same object with slight differences in angle of the shot, perspective, size, etc. and try and find points of interest from one image in the second one.\n",
    "\n",
    "The first step was to detect points of interest in both the images. These \"features\" were found using a Harris corner detector with non-local maxima suppression. After that, we needed to describe the features to be able to compare them. Our feature descriptor is designed as a 16x16 pixel area around the point of interest, which gets split into smaller squares of quarter the size. We create a histogram of the pixels' orientations within the smaller squares and then put all these histogram vectors together into one big descriptor.\n",
    "\n",
    "To compare two features, we simply do a sum of square differences of each of the vectors' elements and the smaller this distance is, the more similar the two features are. We also implemented a ratio test for closest two features. If the ratio of these is higher than 0.68 (closest_feature / second_closest_feature < 0.68), then the two features are too similar to determine which matches better with confidence and we don't use it as a match. If this isn't true, we consider the comparison to be a match.\n",
    "\n",
    "Using this, we were able to get the following accuracies:\n",
    "Notre Dame: 89 %\n",
    "Mount Rushmore: 86 %\n",
    "Episcopal Gaudi: 9 %\n",
    "\n",
    "We also tried using a bigger feature size (feature descriptor area was 64x64 pixels) which increased our Notre Dame accuracy to 90 %, but the process ran for a several minutes and the yield didn't seem to be worth the time.\n",
    "After analyzing the Episcopal Gaudi image, we hypothesized that we may get better results if we use a stronger Gaussian blur on the images first. After testing this, we did indeed increase our accuracy for the third image to 12 % (using cv2.getGaussianKernel(ksize=5, sigma=3) instead of sigma=1). The accuracy on the other images increased significantly with the blur though, so we decided to keep the sigma=1.\n",
    "\n",
    "The key to increasing our accuracies probably lies in our feature detection. If we replace the Harris corner detection with a provided cheat_interest_points function, the reached accuracies are:\n",
    "Notre Dame: 96 %\n",
    "Mount Rushmore: 99 %\n",
    "Episcopal Gaudi: 100 %\n",
    "\n",
    "This is interesting, it tells us that the most important part of this process is correctly identifying key features in images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
